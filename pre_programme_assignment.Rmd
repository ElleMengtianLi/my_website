---
title: 'MFA 20222 Pre-programme Assignment'
author: "ELLE LI"
date: "8 September 2021"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    toc: yes
    toc_float: yes
---

```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
library(gapminder)
library(here)
library(janitor)
```

## Introduction
I am currently a first-year student at London Business School *Masters in Financial Analysis* programme. I am interested in pursuing a Global Markets Summer Analyst Sales and Trading position.\

## Skills
1. I have been **passionate about Financial Markets** since a young age, and I am following market news about stocks, bonds, currencies, derivatives, and commodities daily. I developed huge interests in *New Energy Vehicles* and *Chips* and have done research into these products and sectors during my internships at Genimous and CITIC.
2. My **analytical and quantitative skills** gained from past study and work experience were excellent. During my placement year at UBS, I was responsible for producing monthly risk reports for London branch and EMEA region, providing high level risk profile and analytical commentaries to London branch Chief Risk Officer.
3. Furthermore, with my **cultural background and significant international exposure** both in the UK and China, I believe I would be able to adapt quickly to dynamic work environment and communicate effectively with colleagues and clients. \

## Hobbies and Interests
My life is fast-paced and varied, and I constantly strive to anticipate for dynamic opportunities and aggressively seek for new challenges. Last summer I performed research into what risk the pandemic posed on financial markets, and the paper was published on The Frontiers of Society, Science, and Technology Journal. In addition, I am willing to push beyond what is required. During my placement year at UBS, I have shortened a task from two hours to one second by creating an Excel VBA button and restructured the monthly and quarterly risk reports to better serve managersâ€™ purpose.\

## Link for My LinkedIn Profile
If you are interested for more stories about me, please go to <http://www.linkedin.com/in/elle-li-2000>.

# Task 2: Gapminder dataset

```{r}
glimpse(gapminder)

head(gapminder, 20)

```

I create the `country_data` and `continent_data` with the code below.

```{r}
country_data <- gapminder %>% 
            filter(country == "China")

continent_data <- gapminder %>% 
            filter(continent == "Asia")
```

First, I create a plot of life expectancy over time for *China*, mapping `year` on the x-axis, and `lifeExp` on the y-axis.

```{r, lifeExp_one_country}
plot1<-ggplot(data = country_data, mapping = aes(x = year, y = lifeExp))+
geom_point() +
geom_smooth(se = FALSE)
```

Next I add a title.

```{r, lifeExp_one_country_with_label}
plot1<- plot1 +
labs(title = "Life Expectancy over Time for China",
x = "Year",
y = "Life Expectancy")
```

Secondly, I produce a plot for all countries in *Asia*.

```{r lifeExp_one_continent}
ggplot(continent_data, mapping = aes(x = year, y = lifeExp , colour= country, group = country))+
geom_point() + 
geom_smooth(se = FALSE)
```

Finally, using the original `gapminder` data, I produce a life expectancy over time graph, grouped by continent.

```{r lifeExp_facet_by_continent}
ggplot(data = gapminder , mapping = aes(x = year, y =  lifeExp, colour= country))+
geom_point() + 
geom_smooth(se = FALSE) +
facet_wrap(~continent) +
theme(legend.position="none")
```

Given these trends, it can be seen that life expectancy rose from 1952 to 2007 across all continents, with differences in the patterns.

From the 1960s to 1970s on, the cardiovascular disease mortality declined, leading a decrease in death cases in many developed countries, such as the United States. CVD mortality rate declined in that period mostly since there was a decline in cigarette smoking and tabacco usage. Furthermore, as medical condition advanced, patients having CVD had been well treated and controlled.

Whilst most continents witnessed a steady increase in life expectancy over the 50 years, there were flat or even decreasing lines for most of African countries at around 52 years in 1980s. That was because the HIV or AIDS epidemic quickly became one of the main causes of death in Africa. Thereafter, life expectancy began to rise again at a lower pace, and it was over 64 years in 2020.

# Task 3: Brexit vote analysis

I will have a look at the results of the 2016 Brexit vote in the UK. First I read the data using `read_csv()` and have a quick glimpse at the data.

```{r load_brexit_data, warning=FALSE, message=FALSE}
brexit_results <- read_csv(here::here("data","brexit_results.csv"))


glimpse(brexit_results)
```

The data comes from [Elliott Morris](https://www.thecrosstab.com/), who cleaned it and made it available through his [DataCamp class on analysing election and polling data in R](https://www.datacamp.com/courses/analyzing-election-and-polling-data-in-r).

The main outcome variable (or y) is `leave_share`, which is the percent of votes cast in favour of Brexit, or leaving the EU. Each row is a UK [parliament constituency](https://en.wikipedia.org/wiki/United_Kingdom_Parliament_constituencies).

To get a sense of the spread, or distribution, of the data, I plot a histogram, a density plot, and the empirical cumulative distribution function of the leave % in all constituencies.

```{r brexit_histogram, warning=FALSE, message=FALSE}

# histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_histogram(binwidth = 2.5)+
  labs(title = "Histogram of Brexit Leave Percentage for All Constituencies",
x = "Share of Leave")

# density plot-- think smoothed histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_density()+
  labs(title = "Density Plot of Brexit Leave Percentage for All Constituencies",
x = "Share of Leave")


# The empirical cumulative distribution function (ECDF) 
ggplot(brexit_results, aes(x = leave_share)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent)+
  labs(title = "ECDF of Brexit Leave Percentage for All Constituencies",
x = "Share of Leave")
  


```

One common explanation for the Brexit outcome was fear of immigration and opposition to the EU's more open border policy, so I check the relationship (or correlation) between the proportion of native born residents (`born_in_uk`) in a constituency and its `leave_share`.

```{r brexit_immigration_correlation}
brexit_results %>% 
  select(leave_share, born_in_uk) %>% 
  cor()
```

The correlation is almost 0.5, which shows that the two variables are positively correlated.

I also create a scatter plot between these two variables using `geom_point`. I also add the best fit line, using `geom_smooth(method = "lm")`.

```{r brexit_immigration_plot}
ggplot(brexit_results, aes(x = born_in_uk, y = leave_share)) +
  geom_point(alpha=0.3) +
  
  geom_smooth(method = "lm") + 
  
  labs(title = "Scatter Plot of Brexit Immigration",
x = "Proportion of Native Born Residents", y = "Share of Leave")+
  
  theme_bw() +
  NULL
```

I revisit all of the plots and add an informative title and axes titles to all plots.

It can be seen from the scatter plot of Brexit Immigration that the best fit line between the two variables leave_share and born_in_uk has a positive slope. Therefore, they have a quite strong positive correlation which can also be seen quantitatively from their almost 0.5 correlation figure.

It can also be seen from the news that England, as the largest constituent country within the nation, provided the largest share of Leave voters and saw the largest ratio for Leave at 53.4% to 46.6%. Whilst in Scotland and Northern Ireland, a majority of citizens chose to remain by 62.0% to 38.0% and 55.8% to 44.2% respectively. There was evidence showing that this was because people that were originally born in the UK tend to have pride and priority for their English national identity and therefore had higher urge to not be part of EU.

However, there were opposing views stating that national identity was not the direct reason for people born in the UK to vote for leave or remain.

# Task 4: Animal rescue incidents attended by the London Fire Brigade

[The London Fire Brigade](https://data.london.gov.uk/dataset/animal-rescue-incidents-attended-by-lfb) attends a range of non-fire incidents (which we call 'special services'). These 'special services' include assistance to animals that may be trapped or in distress. The data is provided from January 2009 and is updated monthly. A range of information is supplied for each incident including some location information (postcode, borough, ward), as well as the data/time of the incidents. We do not routinely record data about animal deaths or injuries.

Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.

```{r load_animal_rescue_data, warning=FALSE, message=FALSE}

url <- "https://data.london.gov.uk/download/animal-rescue-incidents-attended-by-lfb/8a7d91c2-9aec-4bde-937a-3998f4717cd8/Animal%20Rescue%20incidents%20attended%20by%20LFB%20from%20Jan%202009.csv"

animal_rescue <- read_csv(url,
                          locale = locale(encoding = "CP1252")) %>% 
  janitor::clean_names()


glimpse(animal_rescue)
```
I count the number of incidents by year to get a broader picture of the data.

```{r, instances_by_calendar_year}

animal_rescue %>% 
  dplyr::group_by(cal_year) %>% 
  summarise(count=n())

animal_rescue %>% 
  count(cal_year, name="count")

```
Then by animal groups:

```{r, animal_group_percentages}
animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  
  summarise(count = n()) %>% 

  mutate(percent = round(100*count/sum(count),2)) %>% 
  
  arrange(desc(percent))

```

I can see that there are some unknown animals rescued, either due to not recording their names properly or due to poor ability of staff on recognising them.

Finally, let me have a loot at the notional cost for rescuing each of these animals. As the LFB says,

> Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.

There is two things I do:

1. Calculate the mean and median `incident_notional_cost` for each `animal_group_parent`
2. Plot a boxplot to get a feel for the distribution of `incident_notional_cost` by `animal_group_parent`.


Before going on, however, I need to fix `incident_notional_cost` as it is stored as a `chr`, or character, rather than a number.

```{r, parse_incident_cost,message=FALSE, warning=FALSE}

typeof(animal_rescue$incident_notional_cost)

animal_rescue <- animal_rescue %>% 

  mutate(incident_notional_cost = parse_number(incident_notional_cost))

typeof(animal_rescue$incident_notional_cost)

```

Now that incident_notional_cost is numeric, let me quickly calculate summary statistics for each animal group. 


```{r, stats_on_incident_cost,message=FALSE, warning=FALSE}

animal_rescue %>% 
  
  group_by(animal_group_parent) %>% 
  
  filter(n()>6) %>% 
  
  summarise(mean_incident_cost = mean (incident_notional_cost, na.rm=TRUE),
            median_incident_cost = median (incident_notional_cost, na.rm=TRUE),
            sd_incident_cost = sd (incident_notional_cost, na.rm=TRUE),
            min_incident_cost = min (incident_notional_cost, na.rm=TRUE),
            max_incident_cost = max (incident_notional_cost, na.rm=TRUE),
            count = n()) %>% 
  
  arrange(desc(mean_incident_cost))

```
The data is sorted in descending order by mean incident cost. It can be seen from the table that larger animals, such as horse, cow, and deer, have higher mean incident costs, whilst animals that are smaller in size have lower mean incident costs.

On the other hand, the above pattern disappeared for median incident cost. Although horse and cow still costed much higher to rescue, the rest of the animals' median incident costs were evenly distributed with little differences.

In addition, for all types of animals, their median incident costs are less than mean incident costs, showing that the distribution is positively skewed. This indicates that there were some outliers in each animal type that incurred high incident costs which pulled the mean incident costs higher. Generally, median incident cost is a better indicator, since it was not affected by abnormal data.

Finally, let me plot a few plots that show the distribution of incident_cost for each animal group.

```{r, plots_on_incident_cost_by_animal_group,message=FALSE, warning=FALSE}

# base_plot
base_plot <- animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  filter(n()>6) %>% 
  ggplot(aes(x=incident_notional_cost))+
  facet_wrap(~animal_group_parent, scales = "free")+
  theme_bw()

base_plot + geom_histogram()
base_plot + geom_density()
base_plot + geom_boxplot()
base_plot + stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent)



```

From my perspective, the ECDF graph best communicates the variability of the incident_notional_cost values. The cost for rabbit and ferret similarly ranged from 275 to around 350. The cost for cat, snake, hamster, and squirrel similarly ranged from around 300 to 600. The cost for fox, bird, cow, and deer similarly ranged from around 500 to 2000, and the cost for horse ranged from 1000 to 3000, which is the largest sum of cost that ever incurred within these animal groups.

It can be seen from the table that different animals witnessed different patterns in spread of incident costs. Most animals' ECDF curves have positive but reducing slope, showing that most cases incurred a small incidence costs. However, it can be seen that for rabbit and ferret, curves have positive and increasing slope, implying that the incidence counts increased at a higher rate as incidence costs increased. Cow also had a different distribution, where the curve is almost a straight line, which means incidence cases were evenly spread across costs.

## Details

-   Who did you collaborate with: NONE
-   Approximately how much time did you spend on this problem set: 2 HOURS
-   What, if anything, gave you the most trouble: THE EXPLANATION PART
